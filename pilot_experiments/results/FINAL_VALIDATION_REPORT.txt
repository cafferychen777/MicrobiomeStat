═══════════════════════════════════════════════════════════════════════════
                   LinDA v2 Pilot Experiments - Final Report

  Generated: 2025-12-24 04:20 UTC
  All experiments completed with R 4.3.3 + Python 3.11
═══════════════════════════════════════════════════════════════════════════

EXECUTIVE SUMMARY
───────────────────────────────────────────────────────────────────────────

Status: MIXED RESULTS - One Success, Two Failures

This report presents ACTUAL experimental results (not simulations) from
testing two proposed innovations for LinDA v2:
  1. Trajectory-aware temporal modeling
  2. Phylogenetic smoothing for weak signal recovery

KEY FINDING: Only one approach validated successfully (Python trajectory test).
The other implementations need refinement before deployment.


═══════════════════════════════════════════════════════════════════════════
  EXPERIMENT 1A: TRAJECTORY MODELING (Python - SUCCESSFUL ✓)
═══════════════════════════════════════════════════════════════════════════

Implementation: Permutation-based trajectory test in Python

Results Summary:

  Metric                         Simple Test      Trajectory Test
  ─────────────────────────────────────────────────────────────────────────
  True Positives                 10/10            10/10
  False Positives                15               3
  Sensitivity (Power)            100.0%           100.0%
  False Discovery Rate           60.0%            23.1%
  Total Significant              25               13

  Key Metrics:
    - Power Improvement: 0% (both at ceiling due to strong signal)
    - FDR Improvement: 61.5% (3x fewer false discoveries!)
    - False Positive Reduction: 80% (15 → 3)

✓ VALIDATION SUCCESSFUL

Interpretation:
  The trajectory-aware test achieved identical statistical power (100%)
  while dramatically reducing false discoveries. This validates the core
  concept: testing for PATTERN differences (trajectories) is more specific
  than testing for overall mean differences.

  The inverted-U signal (spike at middle timepoints, t=2,3) gets "diluted"
  when averaged across all timepoints. Simple mean tests detect both:
    (a) True non-linear signals [correct]
    (b) Random noise that varies across timepoints [false positive]

  Trajectory tests are more stringent because they require evidence of
  coordinated temporal patterns, not just differences at arbitrary timepoints.


═══════════════════════════════════════════════════════════════════════════
  EXPERIMENT 1B: SPLINE LMM (R Implementation - FAILED ✗)
═══════════════════════════════════════════════════════════════════════════

Implementation: Y ~ Group * ns(Time, df=3) + (1|Subject) using lmerTest

Results Summary:

  Metric                         Linear LMM       Spline LMM
  ─────────────────────────────────────────────────────────────────────────
  True Positives                 10/10            0/10
  False Positives                5                5
  Sensitivity (Power)            100.0%           0.0%
  False Discovery Rate           33.3%            100.0%
  Total Significant              15               5

  Power Improvement: -100% (complete failure!)

✗ VALIDATION FAILED

Root Cause Analysis:
  Warnings during execution:
    "fixed-effect model matrix is rank deficient so dropping 2 columns"

  Problem: Over-parameterization
    - Spline basis (3 df) + Group interaction = too many parameters
    - With only 5 timepoints, there's insufficient data
    - Model becomes rank deficient (singular design matrix)
    - Results are unstable/unreliable

  Why Python version succeeded but R version failed:
    Python: Used simple permutation test (non-parametric, robust)
    R: Used parametric spline LMM (requires more data, fragile)

Recommendations:
  1. Reduce spline complexity (df=2 instead of 3)
  2. Use polynomial terms instead of splines
  3. Increase number of timepoints in simulation (7-10)
  4. Consider simpler piecewise linear models
  5. Or use the validated Python permutation approach


═══════════════════════════════════════════════════════════════════════════
  EXPERIMENT 2: PHYLOGENETIC SMOOTHING (R - INCONCLUSIVE ⚠)
═══════════════════════════════════════════════════════════════════════════

Implementation: Graph Laplacian smoothing with lambda=0.5

Simulation Setup:
  - 100 subjects/group (large sample for weak signal detection)
  - 100 ASVs with phylogenetic tree
  - 10 ASVs in signal clade (weak coordinated signal, d=0.3)
  - 90 background ASVs (null)

Results Summary:

  Metric                         No Smoothing     With Smoothing
  ─────────────────────────────────────────────────────────────────────────
  True Positives                 7/10             3/10
  False Positives                3                0
  Sensitivity (Power)            70.0%            30.0%
  False Discovery Rate           30.0%            0.0%
  Total Significant              10               3

  Sensitivity Improvement: -57.1% (worse, not better!)
  FDR Improvement: 100% (perfect precision, but low recall)

⚠ RESULTS INCONCLUSIVE - Trade-off not favorable

Analysis:
  Smoothing did reduce FDR to 0% (perfect precision), BUT:
    - Lost 4 out of 7 true positives (57% sensitivity loss)
    - This trade-off is unfavorable for discovery-phase research
    - Too conservative - misses real biological signals

  Lambda Tuning Results:
    Lambda  Sensitivity  FDR     Interpretation
    ────────────────────────────────────────────────────────────
    0.0     70%          30%     No smoothing (baseline)
    0.1     60%          14%     Slight improvement
    0.25    50%          17%     Marginal
    0.5     30%          0%      Too conservative (default)
    0.75    30%          0%      Too conservative
    1.0     0%           0%      Over-smoothed (no detections)
    2.0     0%           0%      Completely smoothed out

  Optimal lambda appears to be around 0.1-0.25 (not 0.5)

Possible Reasons for Poor Performance:
  1. Signal too weak (d=0.3) for the amount of smoothing
  2. Lambda=0.5 too aggressive for this data structure
  3. Phylogenetic distances may not match signal correlations
  4. Small clade size (10 ASVs) insufficient for borrowing strength

Recommendations:
  1. Re-run with lambda=0.1 or 0.2 (lighter smoothing)
  2. Increase signal strength to d=0.5-0.8
  3. Increase signal clade size to 20-30 ASVs
  4. Test with different tree structures
  5. Consider adaptive lambda selection (cross-validation)


═══════════════════════════════════════════════════════════════════════════
  OVERALL ASSESSMENT AND RECOMMENDATIONS
═══════════════════════════════════════════════════════════════════════════

Summary of Validations:

  Innovation                    Python    R         Overall Status
  ───────────────────────────────────────────────────────────────────────
  Trajectory Modeling           ✓ Pass    ✗ Fail    ✓ Concept Validated
  Phylogenetic Smoothing        N/A       ⚠ Mixed   ⚠ Needs Refinement

RECOMMENDATION: PHASED, CAUTIOUS IMPLEMENTATION

  Phase 1 (Immediate - 1-2 months):
    ✓ Implement trajectory-aware testing (non-parametric approach)
    ✓ Use permutation-based methods (like Python version)
    ✓ Avoid over-parameterized spline models
    ✓ Target: Reduce FDR by 50-70% in real applications

  Phase 2 (Future - Requires More R&D):
    ⚠ Defer phylogenetic smoothing until better validated
    ⚠ Need to solve lambda selection problem
    ⚠ Consider as optional/advanced feature
    ⚠ More extensive simulation studies required

  DO NOT IMPLEMENT:
    ✗ Spline LMM as currently formulated (over-parameterized)
    ✗ Phylogenetic smoothing with fixed lambda=0.5 (too aggressive)


═══════════════════════════════════════════════════════════════════════════
  DETAILED TECHNICAL DISCUSSION
═══════════════════════════════════════════════════════════════════════════

Why Different Implementations Gave Different Results?

1. Trajectory Modeling:

  Python (Successful):
    - Permutation test on sum-of-squared-differences
    - Non-parametric, distribution-free
    - Robust to model mis-specification
    - Tests: "Are trajectories different?" (broad question)

  R (Failed):
    - Parametric spline-based mixed model
    - Assumes specific functional form (natural splines)
    - Sensitive to over-parameterization
    - Tests: "Do spline coefficients differ?" (narrow question)

  Lesson: For small number of timepoints (≤5), non-parametric methods
          are more robust than parametric spline models.

2. Phylogenetic Smoothing:

  Why it reduced power:
    - Smoothing pulls estimates toward neighbors
    - If signal is weak (d=0.3), smoothing can pull true signals
      toward zero faster than it amplifies them
    - Trade-off: Precision ↑, Recall ↓

  When smoothing helps vs. hurts:
    Helps when: - Many weak, correlated signals
                - High noise-to-signal ratio
                - Dense phylogenetic clustering

    Hurts when: - Few isolated signals
                - Signals already moderate strength
                - Sparse phylogenetic distribution

  For this simulation: Signal clade was too small (10 ASVs) and
                       too isolated for smoothing to aggregate
                       enough evidence.


═══════════════════════════════════════════════════════════════════════════
  IMPLEMENTATION GUIDELINES FOR LinDA v2
═══════════════════════════════════════════════════════════════════════════

Recommended Approach for Trajectory Modeling:

  Option A (Recommended - Non-parametric):
    ```r
    # For each feature:
    # 1. Calculate group-specific trajectory means at each timepoint
    # 2. Compute sum of squared differences: SSD = Σ(mean_trt[t] - mean_ctrl[t])^2
    # 3. Permutation test: shuffle group labels, recalculate SSD, repeat 1000x
    # 4. P-value = proportion of permuted SSD >= observed SSD
    ```

    Pros: - Robust, no distributional assumptions
          - Works with any number of timepoints
          - Easy to implement and explain

    Cons: - Computationally intensive (1000 permutations per feature)
          - Cannot easily adjust for covariates

  Option B (Alternative - Piecewise linear):
    ```r
    # Divide time into periods: early, middle, late
    # Test for group differences in each period
    # Combine using omnibus chi-square test
    ```

    Pros: - More interpretable than splines
          - Can use standard mixed models
          - Adjustable for covariates

    Cons: - Requires choosing period boundaries
          - Less flexible than true non-linear models

  DO NOT USE: High-df spline models with few timepoints

Recommendations for Phylogenetic Smoothing (if implemented):

  1. Adaptive lambda selection:
     - Cross-validation to choose lambda per dataset
     - Or use BIC/AIC for model selection

  2. Conditional smoothing:
     - Only smooth if initial evidence of phylogenetic clustering
     - Test for phylogenetic signal before applying smoothing

  3. User control:
     - Make smoothing optional (default: OFF)
     - Provide guidance on when to use it
     - Show both smoothed and unsmoothed results

  4. Better documentation:
     - Clearly explain trade-offs (precision vs. recall)
     - Provide examples where it helps vs. hurts


═══════════════════════════════════════════════════════════════════════════
  LESSONS LEARNED
═══════════════════════════════════════════════════════════════════════════

1. Simple Methods Often Win:
   The non-parametric permutation test outperformed the sophisticated
   spline LMM because it made fewer assumptions and was more robust.

2. Validation Prevents Deployment of Broken Methods:
   Without these pilot experiments, we might have implemented spline
   LMM and wasted months before discovering it doesn't work.

3. Trade-offs Must Be Acceptable:
   Phylogenetic smoothing improved FDR but hurt sensitivity too much.
   Users won't accept losing 57% of true discoveries to eliminate
   false positives.

4. Implementation Details Matter:
   The SAME IDEA (trajectory modeling) succeeded in Python but failed
   in R due to implementation choices. The idea is sound, but requires
   careful implementation.

5. Pilot Experiments Are Essential:
   - 10 minutes of simulation >> months of wasted development
   - Better to discover problems now than after deployment
   - Iterative refinement is necessary


═══════════════════════════════════════════════════════════════════════════
  GENERATED OUTPUTS
═══════════════════════════════════════════════════════════════════════════

Python Implementation (Experiment 1A):
  ✓ pilot_experiments/results/01a_true_signal_pattern.pdf (Python)
  ✓ pilot_experiments/results/01b_pvalue_comparison.pdf (Python)
  ✓ pilot_experiments/results/01c_power_fdr_curves.pdf (Python)
  ✓ pilot_experiments/results/01_detailed_results.csv (Python)
  ✓ pilot_experiments/results/01_summary.json (Python)

R Implementation (Experiments 1B, 2):
  ✓ pilot_experiments/results/01a_true_signal_pattern.pdf (R - updated)
  ✓ pilot_experiments/results/01b_pvalue_comparison.pdf (R - updated)
  ✓ pilot_experiments/results/01c_power_fdr_curves.pdf (R - updated)
  ✓ pilot_experiments/results/01_summary.rds (R)
  ✓ pilot_experiments/results/02_detailed_results.csv
  ✓ pilot_experiments/results/02b_z_score_boost.pdf
  ✓ pilot_experiments/results/02c_pvalue_scatter.pdf
  ✓ pilot_experiments/results/02e_lambda_tuning.pdf
  ✓ pilot_experiments/results/02_summary.rds

Code:
  ✓ pilot_experiments/01_trajectory_spline_validation_v2.py (Python, successful)
  ✓ pilot_experiments/01_trajectory_spline_validation.R (R, needs refinement)
  ✓ pilot_experiments/02_phylo_smoothing_validation.R (R, needs tuning)


═══════════════════════════════════════════════════════════════════════════
  FINAL RECOMMENDATIONS
═══════════════════════════════════════════════════════════════════════════

1. PROCEED with LinDA v2 development, but with modifications:

   Implement:
     ✓ Trajectory-aware testing using permutation approach
     ✓ Non-parametric methods for temporal dynamics
     ✓ Clear documentation of when to use trajectory vs. simple tests

   DO NOT implement (yet):
     ✗ Spline LMM as currently formulated
     ✗ Phylogenetic smoothing with current parameters
     ✗ Any method that failed validation

2. Timeline estimate (conservative):
   - Core trajectory algorithm: 2-3 weeks
   - Testing & validation: 2-3 weeks
   - Real data benchmarking: 3-4 weeks
   - Manuscript preparation: 4-5 weeks
   Total: 3-4 months to submission-ready

3. Next steps:
   a) Refine spline approach (reduce df, test with more timepoints)
   b) Optimize phylogenetic smoothing (lambda selection, larger clades)
   c) Additional simulation studies with varying parameters
   d) Benchmark on real published datasets

4. Paper strategy:
   - Lead with successful trajectory approach (Python validation)
   - Discuss spline approach as "future work" or "alternative"
   - Mention phylogenetic smoothing as "exploratory"
   - Be honest about validation process and negative results
   - Emphasize robustness of permutation-based methods


═══════════════════════════════════════════════════════════════════════════
  CONCLUSION
═══════════════════════════════════════════════════════════════════════════

  Mixed Results: 1 Success, 2 Failures/Inconclusive

  The good news:
    ✓ Core concept validated (trajectory-aware testing works)
    ✓ 80% reduction in false positives is substantial
    ✓ Implementation path is clear (non-parametric permutation)

  The challenges:
    ✗ Parametric implementation needs work
    ⚠ Phylogenetic smoothing needs refinement
    ⚠ More validation studies required

  Overall verdict: PROCEED, but with CAUTION and ITERATION

  This pilot study achieved its goal: prevent deployment of broken
  methods while validating promising approaches. The trajectory-aware
  testing showed clear benefits and should be implemented. The other
  approaches need more R&D before deployment.

═══════════════════════════════════════════════════════════════════════════
  END OF REPORT
═══════════════════════════════════════════════════════════════════════════
